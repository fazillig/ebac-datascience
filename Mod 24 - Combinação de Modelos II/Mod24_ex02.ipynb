{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6f55a58-2926-446a-bd64-aaa8f0964b90",
   "metadata": {},
   "source": [
    "# Tarefa 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4daee4-effe-4dd5-8dbd-d8da106d6660",
   "metadata": {},
   "source": [
    "### 1. Cite 5 diferenças entre GBM e Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43042b3-988c-423b-a60b-c1bc9424eef5",
   "metadata": {},
   "source": [
    "<div style=\"background: #EEEEF4; padding: 15px 20px; border-radius: 5px; margin-top: 20px\">\n",
    "    <p style=\"font-size: 11px; color: #666; font-weight: bold\">Resposta</p>\n",
    "    <ol>\n",
    "        <li>AdaBoost utiliza um conjunto de Stumps (Árvores de decisão com apenas 2 folhas e 1 nível de profundidade) enquanto GBM utiliza árvores de decisão completas</li>\n",
    "\n",
    "<li>No AdaBoost o primeiro passo é um stump enquanto no GBM é a média do Y</li>\n",
    "\n",
    "<li>No AdaBoost cada resposta tem um peso diferente no calculo final. No GBM todas as respostas possuem multiplicadores em comum (learning rate)</li>\n",
    "\n",
    "<li>AdaBoost é mais sensível a outliers do que GBM</li>\n",
    "\n",
    "<li>O processo de treinamento em AdaBoost pode ser mais lento do que GBM devido à natureza sequencial do modelo.</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01cd706-2dd5-42a1-8971-b2bd96a1abb5",
   "metadata": {},
   "source": [
    "### 2. Implemente um modelo AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67900d13-99d4-43b9-944a-cee49585dd30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b3e56be-83a1-40b4-90e0-7f2b4c93d673",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9533333333333334"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "gbm = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1)\n",
    "\n",
    "scores = cross_val_score(gbm, X, y, cv=5)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2afe18a-8caa-4aed-9dd4-c3ad0ed73410",
   "metadata": {},
   "source": [
    "### 3. Cite 4 Hyperparametros importantes no GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed71ed6-8963-4dce-b060-a9b2679a51f1",
   "metadata": {},
   "source": [
    "<div style=\"background: #EEEEF4; padding: 15px 20px; border-radius: 5px; margin-top: 20px\">\n",
    "    <p style=\"font-size: 11px; color: #666; font-weight: bold\">Resposta</p>\n",
    "    <ul>\n",
    "        <li><b>n_estimators</b>: o número de árvores de decisão usadas no modelo GBM.</li>\n",
    "        <li><b>learning_rate</b>: controla a taxa de aprendizado do modelo, ou seja, a contribuição de cada árvore para o modelo final.</li>\n",
    "        <li><b>max_depth</b>: a profundidade máxima das árvores de decisão do GBM.</li>\n",
    "        <li><b>min_samples_split</b>: o número mínimo de amostras necessárias para dividir um nó interno.</li>\n",
    "        <li><b>loss</b>: a função de perda usada para otimizar o modelo GBM, como \"deviance\" para problemas de classificação binária e multiclasse, ou \"ls\" para problemas de regressão.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08df2230-3f43-4ea1-a35d-ea300f7393ea",
   "metadata": {},
   "source": [
    "### 4. Acessando o artigo do Jerome Friedman (Stochastic) e pensando no nome dado ao Stochastic GBM, qual é a maior diferença entre os dois algoritmos?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ce610a-00ad-47bd-93c5-23ab566f5f2c",
   "metadata": {},
   "source": [
    "<div style=\"background: #EEEEF4; padding: 15px 20px; border-radius: 5px; margin-top: 20px\">\n",
    "    <p style=\"font-size: 11px; color: #666; font-weight: bold\">Resposta</p>\n",
    "    <p>O Stochastic Gradient Boosting (SGB) é uma variação do algoritmo de Gradient Boosting Machine (GBM) que adiciona aleatoriedade ao processo de treinamento, selecionando uma amostra aleatória de dados de treinamento em cada iteração para construir cada árvore.</p>\n",
    "    <p>A maior diferença entre os dois algoritmos é que o SGB usa amostragem aleatória de dados de treinamento, enquanto o GBM usa todo o conjunto de dados de treinamento em cada iteração.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf145acd-34be-4480-b52b-b83dc541c9f9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
